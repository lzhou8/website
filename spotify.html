<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Lillie Zhou</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
   <script src="https://kit.fontawesome.com/268b8bb572.js"></script>
   <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/4.9.1/d3.min.js"></script>-->
   <!--<script src="https://d3js.org/d3.v4.js" charset="utf-8"></script>-->
   <script src="https://d3js.org/d3.v4.js"></script>
   <!--<script src="https://unpkg.com/d3@4.13.0/build/d3.min.js"></script>-->
   <link rel="stylesheet" href="css/flight-delays/styles.css">
</head>

<body>
  <div class="jumbotron jumbotron-fluid" style="background: black;">
    <div class="container">
      <h1 align="center" style = "color: lightgreen;">Spotify Music Analysis</h1>
      <div align="center">
        <p class="lead" style="color: white;">“Lofi”, or low fidelity, is a type of music genre that is characterized by its deliberate, aesthetic use of "production flaws" and distorted instrumentals. The lack of broader analysis on the Internet about this genre of music, as well as my recent discovery of Spotify’s developer APIs, inspired me to create some data visualizations to assess its audio features. </p>
      </div>
    </div>
  </div>

  <div class="container">
    <h2 align="left">Audio Features</h2>

    <p>
      According to Spotify’s API documents, audio can be evaluated based on several quantitative features:
      <ul>
          <li>Danceability</li>
          <li>Instrumentalness</li>
          <li>Energy</li>
          <li>Loudness</li>
          <li>Speechiness</li>
          <li>Valence</li>
          <li>Acousticness</li>
      </ul>

      With the exception of loudness (which is assessed in decibels), these audio features are assessed on a continuous scale of 0 to 1, where 0 is the lowest and 1 the highest. A Spotify track with a speechiness of 0.9, for example, is considered to have a high amount of spoken utterances. 
    </p>
  </div>

  <div class="container">
    <h2 align="left">Data Collection</h2>

    <p>
      Spotipy is a Python library that allows access to Spotify's developer APIs. With Spotipy, I wrote a Python utility that retrieves all the tracks that have been produced by the artist "Lofi ChillHop". Each track (along with its associated audio features) was written to a CSV file. 
    </p>
  </div>

  <div class="container">
    <h2 align="left">Visualizations</h2>

    <p>
      Using the CSV file, I built several data visualizations to analyze overall trends and patterns in audio features. I chose to build <b>box plots</b> with an overlay of individual points, so that the overall trends for each audio feature could be observed. Hover over an individual point to see its corresponding tooltip!
    </p>
  </div>

  <div class="container">
    <h5 align="left">Boxplots</h5>
    <div id="boxplot2"></div>
    <div id="tooltip"></div>
  </div>

  <div class="container">
    <h2 align="left">Analysis</h2>
    <p>From this visualization, it is observed that the Lofi tracks in this dataset trend towards high acousticness, high danceability, low to mid energy, high instrumentality, low liveness, low speechiness, and low to mid valence. As the Lofi music genre is generally characterized by its heavy use of instrumentals (and by extension, few spoken utterances), I had expected most of the tracks in the dataset to be clustered near 1.0 in instrumentality and near 0.0 in speechiness. An interesting result that I had not originally anticipated was the above average metric of danceability. Most of the individual tracks seem to be found in the range between 0.5 and 0.8. This runs contrary to my original perception that the tracks by this artist would trend towards low danceability. 
    <br><br>
    Looking at the interquartile ranges, I notice that the "spread" of the data points for audio features such as liveness, speechiness, and danceability are small, while the "spread" for acousticness and valence are, by comparison, large. This informs that the middle 50% for acousticness and valence are more variable, and the quantiative values for these 2 audio features falls across a wider spectrum. With audio features such as liveness and speechiness, the outliers are more clearly defined compared to the other features, where there are not so much outliers but as clusters of tracks that are fewer in number than the clusters in the middle 50%. 
    </p>
  </div>
</body>

<script src="js/spotify2.js"></script>
</html>